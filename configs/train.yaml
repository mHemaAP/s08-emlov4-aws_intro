# @package _global_

# specify here default configuration
# order of defaults determines the order in which configs override each other
defaults:
  - _self_
  - data: dogs
  - model: mdogs
  - callbacks: default
  - logger: default # set logger here or use command line (e.g. `python train.py logger=tensorboard`)
  - trainer: default
  - paths: default
    # - hydra: null #default

  # experiment configs allow for version control of specific hyperparameters
  # e.g. best hyperparameters for given model and datamodule
  - experiment: finetune.yaml
  # debugging config (enable through command line, e.g. `python train.py debug=default)
  - debug: null


  - override hydra/sweeper: optuna
  - override hydra/sweeper/sampler: tpe
  - override hydra/launcher: joblib



# task name, determines output directory path
task_name: "train"

# tags to help you identify your experiments
# you can overwrite this in experiment configs
# overwrite from command line with `python train.py tags="[first_tag, second_tag]"`
tags: ["dev"]

# set False to skip model training
train: True

# evaluate on test set, using best model weights achieved during training
# lightning chooses best weights based on the metric specified in checkpoint callback
test: True

# simply provide checkpoint path to resume training
ckpt_path: null

# seed for random number generators in pytorch, numpy and python.random
seed: 42


hydra:
  launcher:
    # don't go above [RunTimeError:: Please call `iter(combined_loader)` first.]
    # `NOT ABLE to UNDERSTAND :(`:: https://github.com/Lightning-AI/pytorch-lightning/issues/19373 
    # No activity:: https://github.com/openvinotoolkit/anomalib/issues/2078
    n_jobs: 1   
  sweeper:
    sampler:
      _target_: optuna.samplers.TPESampler
      seed: 32
      n_startup_trials: 3 # number of random sampling runs before optimization starts
    direction: minimize   # we use test_metrics['test/loss_epoch']
    study_name: optimal_searching
    n_trials: 10  #108 totoal
    n_jobs: 16    # 16-thread
    params:
      # https://github.com/facebookresearch/hydra/discussions/2906
      model.dims: choice([6,12,24,36],[12,24,48,72])  # "[6,12,24,36],[12,24,48,72]"       # [3,6,12,18], [6,12,24,36] 
      model.depths: "[3,3,15,3],[3,4,27,3], [3,3,9,3]"     
      model.head_fn: choice('norm_mlp','default')
      model.conv_ratio: choice(1,1.2,1.5)                   #choice(1,1.2,1.5)
      data.batch_size: choice(16,32,64)